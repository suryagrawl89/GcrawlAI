# Web Crawler API & Dashboard

A comprehensive web crawling solution featuring a FastAPI backend, Streamlit dashboard, and distributed task processing with Celery and Redis.

## ğŸš€ Features

- **Dual Mode Crawling**:
  - **Single Page**: Direct, real-time crawling of individual pages.
  - **Full Site**: Distributed crawling of entire websites using Celery workers.
- **Advanced Extraction**:
  - Markdown conversion for LLM consumption.
  - HTML & Screenshot capture.
  - SEO metadata extraction (Title, Description, Keywords).
- **Stealth & Anti-Blocking**:
  - Uses Playwright with stealth plugins to bypass bot detection.
  - Automatic fallback strategies (Chromium -> Firefox/Camoufox).
- **Real-time Updates**:
  - Live progress tracking via WebSockets.
  - Interactive Streamlit dashboard.
- **Authentication**:
  - JWT-based authentication.
  - Email OTP for signup/verification.
  - Password reset flow.

## ğŸ› ï¸ Technology Stack

- **Backend**: FastAPI, Python 3.9+
- **Frontend**: Streamlit
- **Database**: PostgreSQL
- **Task Queue**: Celery + Redis
- **Browser Automation**: Playwright
- **Authentication**: JWT, BCrypt

## ğŸ“‹ Prerequisites

- **Python 3.9+**
- **PostgreSQL** (running on default port 5432)
- **Redis** (running on default port 6379)
- **Git**

## âš™ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/GramosoftAI/GcrawlAI.git
   cd GcrawlAI
   ```

2. **Create and activate virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install Playwright browsers**
   ```bash
   playwright install
   ```

## ğŸ”§ Configuration

1. **Database Config**: Update `config.yaml` with your PostgreSQL credentials.
   ```yaml
   postgres:
     host: "localhost"
     port: 5432
     database: "crawlerdb"
     user: "postgres"
     password: "your_password"
   ```

2. **Initialize Database Tables**:
   ```bash
   python -m api.db_setup
   # OR
   python api/db_setup.py
   ```

## ï¿½â€â™‚ï¸ Running the Application

You need to run 4 separate processes. It's recommended to use separate terminal windows.

**1. Start Redis Server** (if not running as a service)
```bash
redis-server
```
Note: Redis server will not run on windows, so use WSL for running redis server. or use docker.

**2. Start Celery Worker**
```bash
# Linux (User Recommended)
celery -A web_crawler.celery_config worker -l info

# Windows
celery -A web_crawler.celery_config.celery_app worker --loglevel=info --pool=solo
```

**3. Start Backend API**
```bash
# Windows / Development
uvicorn api.api:app --reload --port 8000

# Linux / Production (User Recommended)
uvicorn api.api:app --host 0.0.0.0 --port 8000 --workers 4 --timeout-keep-alive 120
```
API Docs will be available at: http://localhost:8000/docs

**4. Start Frontend Dashboard**
```bash
cd web_crawler
streamlit run streamlit_app.py
```
Dashboard will be available at: http://localhost:8501

## ï¿½ Project Structure

```
.
â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â”œâ”€â”€ api.py              # Main API entry point
â”‚   â”œâ”€â”€ auth_manager.py     # Authentication logic
â”‚   â””â”€â”€ db_setup.py         # Database initialization
â”œâ”€â”€ web_crawler/            # Crawler logic
â”‚   â”œâ”€â”€ web_crawler.py      # Core crawler orchestrator
â”‚   â”œâ”€â”€ page_crawler.py     # Individual page processing
â”‚   â”œâ”€â”€ streamlit_app.py    # Frontend dashboard
â”‚   â””â”€â”€ celery_config.py    # Celery configuration
â”œâ”€â”€ config.yaml             # Application configuration
â””â”€â”€ requirements.txt        # Python dependencies
```

## ğŸ” API Endpoints

- `POST /crawler`: Start a new crawl job (single or all).
- `GET /crawler/status/{task_id}`: Check Celery task status.
- `GET /crawl/markdown`: Retrieve generated markdown.
- `POST /auth/signup/send-otp`: reliable email-based signup.
- `POST /auth/signup/verify-otp`: reliable email-based signup.
- `POST /auth/signin`: reliable email-based signin.
- `POST /auth/forgot-password`: reliable email-based forgot password.
- `POST /auth/reset-password`: reliable email-based reset password.

